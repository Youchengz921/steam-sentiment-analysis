# train_model.py
# ä½¿ç”¨æ”¶é›†çš„ Steam è©•è«–è³‡æ–™å¾®èª¿æƒ…æ„Ÿåˆ†ææ¨¡å‹

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback
)
import evaluate

# è¨­å®š
TRAINING_DATA_FILE = "training_data.csv"
OUTPUT_MODEL_DIR = "./fine_tuned_model"
BASE_MODEL = "xlm-roberta-base"  # å¤šèªè¨€æ¨¡å‹

def load_data():
    """è¼‰å…¥è¨“ç·´è³‡æ–™"""
    print(f"ğŸ“‚ è¼‰å…¥è³‡æ–™: {TRAINING_DATA_FILE}")
    df = pd.read_csv(TRAINING_DATA_FILE)
    print(f"   ç¸½è³‡æ–™é‡: {len(df)}")
    print(f"   æ­£é¢: {len(df[df['label'] == 1])}, è² é¢: {len(df[df['label'] == 0])}")
    return df

def prepare_datasets(df, tokenizer):
    """æº–å‚™è¨“ç·´å’Œé©—è­‰è³‡æ–™é›†"""
    # åˆ†å‰²è³‡æ–™
    train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
    
    print(f"   è¨“ç·´é›†: {len(train_df)}, é©—è­‰é›†: {len(eval_df)}")
    
    # è½‰æ›ç‚º Dataset
    train_dataset = Dataset.from_pandas(train_df[['text', 'label']].reset_index(drop=True))
    eval_dataset = Dataset.from_pandas(eval_df[['text', 'label']].reset_index(drop=True))
    
    # Tokenize
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, max_length=512)
    
    train_dataset = train_dataset.map(tokenize_function, batched=True)
    eval_dataset = eval_dataset.map(tokenize_function, batched=True)
    
    return train_dataset, eval_dataset

def compute_metrics(eval_pred):
    """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
    accuracy_metric = evaluate.load("accuracy")
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy_metric.compute(predictions=predictions, references=labels)

def main():
    print("=" * 60)
    print("ğŸ¤– Steam è©•è«–æƒ…æ„Ÿåˆ†ææ¨¡å‹å¾®èª¿")
    print("=" * 60)
    
    # æª¢æŸ¥è³‡æ–™æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(TRAINING_DATA_FILE):
        print(f"âŒ æ‰¾ä¸åˆ° {TRAINING_DATA_FILE}")
        print("   è«‹å…ˆåŸ·è¡Œ collect_training_data.py æ”¶é›†è³‡æ–™")
        return
    
    # è¼‰å…¥è³‡æ–™
    df = load_data()
    
    # è¼‰å…¥ tokenizer å’Œæ¨¡å‹
    print(f"\nğŸ“¦ è¼‰å…¥åŸºåº•æ¨¡å‹: {BASE_MODEL}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    model = AutoModelForSequenceClassification.from_pretrained(
        BASE_MODEL,
        num_labels=2,
        id2label={0: "NEGATIVE", 1: "POSITIVE"},
        label2id={"NEGATIVE": 0, "POSITIVE": 1}
    )
    
    # æº–å‚™è³‡æ–™é›†
    print("\nğŸ”§ æº–å‚™è³‡æ–™é›†...")
    train_dataset, eval_dataset = prepare_datasets(df, tokenizer)
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_MODEL_DIR,
        num_train_epochs=5,                   
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        learning_rate=2e-5,                   
        warmup_ratio=0.1,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=100,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        fp16=True,
    )
    
    # è³‡æ–™æ”¶é›†å™¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # å»ºç«‹ Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # 3 epochs æ²’é€²æ­¥å°±åœæ­¢
    )
    
    # é–‹å§‹è¨“ç·´
    print("\nğŸš€ é–‹å§‹è¨“ç·´...")
    print("   é€™å¯èƒ½éœ€è¦ 10-30 åˆ†é˜ï¼Œå–æ±ºæ–¼æ‚¨çš„ç¡¬é«”")
    print("-" * 60)
    
    trainer.train()
    
    # è©•ä¼°
    print("\nğŸ“Š è©•ä¼°æ¨¡å‹...")
    eval_results = trainer.evaluate()
    print(f"   é©—è­‰ Accuracy: {eval_results['eval_accuracy']:.4f}")
    
    # å„²å­˜æ¨¡å‹
    print(f"\nğŸ’¾ å„²å­˜æ¨¡å‹è‡³ {OUTPUT_MODEL_DIR}")
    trainer.save_model(OUTPUT_MODEL_DIR)
    tokenizer.save_pretrained(OUTPUT_MODEL_DIR)
    
    print("\n" + "=" * 60)
    print("âœ… è¨“ç·´å®Œæˆï¼")
    print(f"   æ¨¡å‹å·²å„²å­˜è‡³: {OUTPUT_MODEL_DIR}")
    print("   æ‚¨ç¾åœ¨å¯ä»¥åœ¨ app.py ä¸­ä½¿ç”¨é€™å€‹æ¨¡å‹äº†")
    print("=" * 60)

if __name__ == "__main__":
    main()
